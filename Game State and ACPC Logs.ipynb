{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards\n",
    "max_players = 10\n",
    "num_players = 2\n",
    "community_pool = 1 #might consider community cards as \"dealer\" along with pot\n",
    "suits = 4\n",
    "card_ranks = 13\n",
    "\n",
    "#rounds\n",
    "max_betting_rounds = 4 #pre-flop, flop, turn, river\n",
    "max_raises = 4\n",
    "\n",
    "#betting - won't see this in ACPC server text. Supposed to be known by program\n",
    "small_blind = 50\n",
    "big_blind = 100\n",
    "min_raise = 100 #big blind or minimum of previous raise/bet in same round\n",
    "\n",
    "stack_sizes = 20000 #could be a list for each player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of game state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibilities: one funnel for cards: [suit x rank x (players + community + dealer) x round] \n",
    "and one for actions / positions [players x rounds x max raises]\n",
    "...or throw it all in one? (players + community + dealer) x (suit x rank) x rounds x max raises. a 5D tensor.\n",
    "\n",
    "another option: Players (and dealer) x cards x rounds x action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496\n"
     ]
    }
   ],
   "source": [
    "size = 3 * 4 * 13 * 4 * 4 #players x suits x rank x betting rounds x max number of raises \n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACPC example record: STATE:0:r300c/cc/cc/r749f:Qs8h|QcTd/Jd6hAd/Jh/6c:300|-300:Intermission_2pn_2017|HITSZ_2pn_2017\n",
    "\n",
    "suits = {'s':0 , 'c': 1, 'h': 2, 'd': 3}\n",
    "cardRanks =\\\n",
    "{\"2\": 0,\n",
    "\"3\": 1,\n",
    "\"4\": 2,\n",
    "\"5\":3,\n",
    "\"6\":4,\n",
    "\"A\":5,\n",
    "\"7\":6,\n",
    "\"8\":7,\n",
    "\"9\":8,\n",
    "\"10\":9,\n",
    "\"J\":10,\n",
    "\"Q\":11,\n",
    "\"K\":12,\n",
    "\"A\": 13}\n",
    "\n",
    "players = np.zeros(max_players + 1) # +1 is for dealer cards and pot\n",
    "rounds = np.zeros(max_betting_rounds) \n",
    "action = np.zeros(max_raises + 1) # 1 round of betting or checking + raising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"next\" state in poker for Q learning? Is it the next time an action opportunity occurs for the agent? I think yes\n",
    "but it could also mean next state of the game, not necessarily agent to act. Need to confirm, but I think next-action opportunity makes sense, because otherwise how would you know the maximum best next move from the target model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to know the \"agent\" i.e. the hero\n",
    "hero = 0 #this is for one of the states, can also get the states for players up to num_players\n",
    "#could assign hero via position, or assign position via hero... I think I prefer assign position to agent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
